# DV_01 Challenge #

## Challenge ##
Files from lending club were provided via google drive, and in two hours we were asked to clean and analyze the data, with a sample chart given to replicate. Upon some brief data exploration, I decided to attempt to replicate the given chart. First I used the dv01_challenge.ipynb notebook to briefly explore the data. I was also able to find a data dictionary available online [here](https://resources.lendingclub.com/LCDataDictionary.xlsx#:~:text=LoanStats&text=The%20number%20of%20accounts%20on%20which%20the%20borrower%20is%20now%20delinquent.&text=Number%20of%20trades%20opened%20in%20past%2024%20months.).

I was able to mostly replicate the chart in the given time, with the exception of the Adj. Net Annualized Return column. There are small deviations with the Charged Off (Net) and Interest Payments Received columns, roughly 3.4k, with my calculation underestimating for both. This makes me think there may be either some intricacy in which certain loans had additional fields which would contribute to charge offs and interest. For several of the grades the numbers match exactly, which is why I believe it to be only for certain loans. There is also the potential that the data has changed in a minor way between the sample chart and the data provided.

## Solution ##
After exploring the data and coming up with the appropriate data transformations required in the notebook, I created dv01_challenge.py to create the charts. I used glob to pull all csv files from the data folder, so as long as each individual csv file fits into memory, this will scale. The final output file is simply the input file with the extension .md added to the end. I considered extracting the date from the input file, but in case we wanted to use this solution with different groups for the same quarter, or switch to monthly files, or any other change in aggregation, this would continue to work. There were 3 input files, so there are 3 markdown files available in the data section.

## Future Improvements ##
Given additional time, there are some improvements that could be made to the process:
- Quality checks for input files. It's possible the schema may change for future files and we may want to ensure that the columns we use exist and allowable values are the same. For example if in a future file the grades were lowercase, the FG aggregation would fail as written without a notification of error. We would catch this when reviewing the output, but it would be better practice to be notified of the error prior to QCing output, particularly if the data starts to get large and processing takes a long time
- Option to create table in chunks if files get large. We'd likely want to use something like spark instead if the files got very large, but this approach would still work with big data if we create the option to chunk the files, compute summaries for each chunk, and then aggregate into the final table. Other than average interest rate this would simply be sums, while for interest rate we would need to compute the numerator and denominator for each chunk, and then divide the sum of numerators by sum of denominators in the final aggregation
- Research the charged off and interest payments discrepency. Whether we could discuss with lending club or whoever the client is that creates the GUI, or if there were an internal data expert to discuss with, it would be worth looking into why we are not matching for some of the grades. Is there a payment other than interest and late fees that we would not consider to be reducing the principal? Are there other line items that would be considered part of interest payments that we did not consider?
- Understand why our principal payments received column, even when charge offs are correct, are not matching the total_rec_prncp column, which is defined in the data dictionary as "Principal received to date". Understand whether the GUI is correct whether it's worth discussing the correct approach for reporting the principal payments received.
